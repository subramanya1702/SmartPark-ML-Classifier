# -*- coding: utf-8 -*-
"""PLV_MODULE1_PIPELEINE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vHs5S8M8Siu6Vdqa2T74fzWEpjaL8ZN7
"""
import base64
import configparser
import os
import pathlib
import subprocess
import time
import uuid

import boto3

from my_utils.image_fetcher import image_fetcher
from my_utils.preprocess3 import pre_process


def process2_pipeline(MODEL, PREPROCESSING_CONFIG, original_object_folder, prediction_object_folder):
    url = "https://www.youtube.com/watch?v=e9LYewJGQlk"

    # IMAGE FETCHER SCRIPT
    fetched_image, filename_string, timestamp = image_fetcher(url)

    # Setting the path for saving the fetched and preprocessed images.
    original_image_dir_path = PREPROCESSING_CONFIG["original-image"]
    preprocess_image_dir_path = PREPROCESSING_CONFIG["preprocessed-image-dir-save-path"]

    # Sending the image to preprocessing script
    pre_processed_image = pre_process(fetched_image, original_image_dir_path, preprocess_image_dir_path,
                                      filename_string)

    # Setting the directory path to fetch the image for detection
    source = preprocess_image_dir_path + filename_string

    # PREDICTION part of the process pipeline
    command = [
        'python3',
        'detect.py',
        '--weights',
        MODEL["weights"],
        '--source',
        source,
        '--img-size',
        MODEL["img-size"],
        '--conf-thres',
        '0.68'
    ]

    # Running the detection script
    sp = subprocess.Popen(command, stdout=subprocess.PIPE)
    output, _ = sp.communicate()

    # POST-PROCESSING Script
    print(output)
    result_string = (output.split(b'\n'))[-4]
    print(result_string)
    try:
        number_of_vehicles = int(result_string.split(b" ")[1])
        number_of_empty_parking_slots = 41 - number_of_vehicles
    except:
        print("Exception while processing the model output. Setting the number of available parking slots to 0")
        number_of_empty_parking_slots = 41

    # Upload original image to S3
    original_object = original_object_folder + filename_string
    pre_processed_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(),
                                                 f"pre_processed_image/{filename_string}")
    response = s3.upload_file(pre_processed_image_file_name, bucket_name, original_object)
    print(response)
    original_http_url = "https://%s.s3.amazonaws.com/%s" % (bucket_name, original_object)
    print(original_http_url)

    # Upload prediction/annotated image to S3
    prediction_object = prediction_object_folder + filename_string
    output_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(),
                                          f"inference/output/{filename_string}")
    response = s3.upload_file(output_image_file_name, bucket_name, prediction_object)
    print(response)
    prediction_http_url = "https://%s.s3.amazonaws.com/%s" % (bucket_name, prediction_object)
    print(prediction_http_url)

    # Clean up local images after uploading to S3
    input_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(), f"images/{filename_string}")
    os.remove(pre_processed_image_file_name)
    os.remove(input_image_file_name)
    os.remove(output_image_file_name)

    # Insert to DynamoDb
    str_timestamp = str(int(timestamp))
    common_request_body = {
        'latlon': '44.56298278509426:-123.27235573138302',
        'parking_lot_name': 'Tebeau Hall',
        'empty_parking_spaces': str(number_of_empty_parking_slots),
        'total_parking_spaces': '41',
        'timestamp': str_timestamp,
        'original_image_url': original_http_url,
        'prediction_image_url': prediction_http_url,
        'parking_lot_time_limit': "2 Hr Parking [ 8.30 am to 5.30 am]",
        'parking_charges': 'Pay at Pay Station [ 2$ per Hr ]'
    }

    # Insert the record to AllParkingLots table
    # This is the table which is actually used by lambda to serve the response
    all_parking_lots_table = dynamodb.Table('AllParkingLots')
    all_parking_lots_table.put_item(
        Item=common_request_body
    )

    # Insert the record to ParkingLotLogs table
    # This table is solely used for log keeping purposes
    parking_lot_logs_table = dynamodb.Table('ParkingLotLog')
    common_request_body['id'] = str(uuid.uuid4())
    common_request_body['ttl'] = int(timestamp)
    parking_lot_logs_table.put_item(
        Item=common_request_body
    )

    print("Records inserted to dynamodb")


if __name__ == "__main__":
    # Read config.ini file
    config_obj = configparser.ConfigParser()
    config_obj.read("config/config.ini")

    MODEL = config_obj["MODEL_INPUTS"]
    AWS_CREDENTIALS = config_obj["AWS_CREDS"]
    AWS_ENV = config_obj["AWS_ENV"]
    PREPROCESSING_CONFIG = config_obj["PRE_PROCESSING_BOUNDS"]
    ACCESS_KEY = base64.b64decode(AWS_CREDENTIALS["access_key"]).decode('utf-8')
    SECRET_KEY = base64.b64decode(AWS_CREDENTIALS["secret_key"]).decode('utf-8')

    bucket_name = AWS_ENV["s3_bucket_name"]
    original_object_name = AWS_ENV["s3_original_object_name"]
    prediction_object_name = AWS_ENV["s3_prediction_object_name"]

    s3 = boto3.client("s3", aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)
    dynamodb = boto3.resource("dynamodb", aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY,
                              region_name="us-east-1")

    n = 0
    while True:
        print(f"Iteration {n}")
        start = time.time()

        process2_pipeline(MODEL, PREPROCESSING_CONFIG, original_object_name, prediction_object_name)

        end = time.time()
        diff = end - start
        print(diff)
        time.sleep(3)
        n += 1
