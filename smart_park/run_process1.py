# -*- coding: utf-8 -*-
"""PLV_MODULE1_PIPELEINE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vHs5S8M8Siu6Vdqa2T74fzWEpjaL8ZN7
"""
import base64
import os
import pathlib
import subprocess
import time
import uuid

import boto3
import yaml

from my_utils.image_fetcher import image_fetcher
from my_utils.preprocess3 import pre_process


def process2_pipeline(parking_lot, MODEL, PREPROCESSING_CONFIG, original_object_folder, prediction_object_folder):
    # IMAGE FETCHER SCRIPT
    fetched_image, filename_string, timestamp = image_fetcher(parking_lot["source_url"])

    # Setting the path for saving the fetched and preprocessed images.
    original_image_dir_path = PREPROCESSING_CONFIG["original-image"]
    preprocess_image_dir_path = PREPROCESSING_CONFIG["preprocessed-image-dir-save-path"]

    # Sending the image to preprocessing script
    pre_process(PREPROCESSING_CONFIG, fetched_image,
                original_image_dir_path,
                preprocess_image_dir_path,
                filename_string)

    # Setting the directory path to fetch the image for detection
    source = preprocess_image_dir_path + filename_string

    # PREDICTION part of the process pipeline
    command = [
        'python3',
        'detect.py',
        '--weights',
        MODEL["weights"],
        '--source',
        source,
        '--img-size',
        str(MODEL["img-size"]),
        '--conf-thres',
        '0.68'
    ]

    # Running the detection script
    sp = subprocess.Popen(command, stdout=subprocess.PIPE)
    output, _ = sp.communicate()

    # POST-PROCESSING Script
    print(output)
    result_string = (output.split(b'\n'))[-4]
    print(result_string)
    try:
        number_of_vehicles = int(result_string.split(b" ")[1])
        number_of_empty_parking_slots = parking_lot["spaces"] - number_of_vehicles
    except Exception:
        print("Exception while processing the model output. Setting the number of available parking slots to 0")
        number_of_empty_parking_slots = parking_lot["spaces"]

    # Upload original image to S3
    original_object = original_object_folder + filename_string
    pre_processed_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(),
                                                 f"pre_processed_image/{filename_string}")
    response = s3.upload_file(pre_processed_image_file_name, bucket_name, original_object)
    print(response)
    original_http_url = "https://%s.s3.amazonaws.com/%s" % (bucket_name, original_object)
    print(original_http_url)

    # Upload prediction/annotated image to S3
    prediction_object = prediction_object_folder + filename_string
    output_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(),
                                          f"inference/output/{filename_string}")
    response = s3.upload_file(output_image_file_name, bucket_name, prediction_object)
    print(response)
    prediction_http_url = "https://%s.s3.amazonaws.com/%s" % (bucket_name, prediction_object)
    print(prediction_http_url)

    # Clean up local images after uploading to S3
    input_image_file_name = os.path.join(pathlib.Path(__file__).parent.resolve(), f"images/{filename_string}")
    os.remove(pre_processed_image_file_name)
    os.remove(input_image_file_name)
    os.remove(output_image_file_name)

    # Insert to DynamoDb
    str_timestamp = str(int(timestamp))
    common_request_body = {
        'latlon': str(parking_lot["latitude"]) + ":" + str(parking_lot["longitude"]),
        'parking_lot_name': parking_lot["name"],
        'empty_parking_spaces': str(number_of_empty_parking_slots),
        'total_parking_spaces': str(parking_lot["spaces"]),
        'timestamp': str_timestamp,
        'original_image_url': original_http_url,
        'prediction_image_url': prediction_http_url,
        'parking_lot_time_limit': parking_lot["time_limit"],
        'parking_charges': str(parking_lot["charges"])
    }

    # Insert the record to AllParkingLots table
    # This is the table which is actually used by lambda to serve the response
    all_parking_lots_table = dynamodb.Table('AllParkingLots')
    all_parking_lots_table.put_item(
        Item=common_request_body
    )

    # Insert the record to ParkingLotLogs table
    # This table is solely used for log keeping purposes
    parking_lot_logs_table = dynamodb.Table('ParkingLotLog')
    common_request_body['id'] = str(uuid.uuid4())
    common_request_body['ttl'] = int(timestamp)
    parking_lot_logs_table.put_item(
        Item=common_request_body
    )

    print("Records inserted to dynamodb")


if __name__ == "__main__":
    # Read and extract configuration from config.yml
    with open('config/config.yml', 'r') as file:
        sp_config = yaml.safe_load(file)

    MODEL = sp_config["model-inputs"]
    AWS_CREDENTIALS = sp_config["aws"]["credentials"]
    AWS_ENV = sp_config["aws"]["environment"]
    PREPROCESSING_CONFIG = sp_config["pre-processing-bounds"]
    ACCESS_KEY = base64.b64decode(AWS_CREDENTIALS["access_key"]).decode('utf-8')
    SECRET_KEY = base64.b64decode(AWS_CREDENTIALS["secret_key"]).decode('utf-8')

    bucket_name = AWS_ENV["s3_bucket_name"]
    original_object_name = AWS_ENV["s3_original_object_name"]
    prediction_object_name = AWS_ENV["s3_prediction_object_name"]

    s3 = boto3.client("s3", aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)
    dynamodb = boto3.resource("dynamodb", aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY,
                              region_name="us-east-1")

    parking_lots = sp_config["parking-lots"]

    n = 0
    while True:
        print(f"Iteration {n}")
        start = time.time()

        # TODO: Execute different parking lots parallely
        for parking_lot in parking_lots:
            process2_pipeline(parking_lot, MODEL, PREPROCESSING_CONFIG, original_object_name, prediction_object_name)

        end = time.time()
        diff = end - start
        print(diff)
        time.sleep(3)
        n += 1
